#+title: Boredealis
#+author: Jason

Using Vision Transformers (ViT) for removing clouds from a video of the Aurora Borealis.

* Methodology

From an email containing the inital proposal:

#+begin_quote
Firstly, I believe Vision Transformers (ViT) to be a suitable technology for this. ViTs are best at cleaning up an image, and the goal of this project is to try and recover the Borealis from behind cloud cover. My belief is that a cloud is nothing more than an opacity layer layered on top of the desired image, literally like layers in Photoshop with a certain opacity value. The clouds can be removed with a learned filter bank, revealing the Aurora from the clouds behind it.

Since ViTs only work on still images, and our data is video, the video has to be converted into a series of still images, and the ViT applied to every image, before it is recompiled back into a video. There are already packages, like FFmpeg, that support this process. It's a simple bash script, which can be called by a Python subprocess.

Once the video is in images, they have to be turned into 256x256 or 512x512 chunks (I will see which one is more suitable once I receive the data) so that a ViT will recognize the input image as a valid size. This is also a simple script. To exclude any non-image frames, I can also discard any frames that are 60% or more of the background black color present in all frames. Stitching the output chunks back into a single image is also a simple task, given the size of the original (which can be easily retrieved).

The hardest part about this project is going to be making training data. Since there are no days where the Aurora is the same, the only option is to "fake" cloud coverage on clear Aurora days. I believe the best and simplest way to simulate clouds is as an opacity layer of different shades of grey. The actual bounds for lower and upper grey I can pull from the training data. Clouds can be "simulated" as a solid image of some shade of grey, layered on top of the clear Aurora image at a certain opacity using an over operator. The image can be generated by PIL.Image, on the fly, in the dataloader. The opacity and shade of grey can be changed in the dataloader using a random range for the training data. To simulate light scattering from the clouds, a small amount of Gaussian blur should suffice. To help prevent overfixing, I will add random Gaussian noise also in the dataloader.
Tl;dr: simulate clouds over a clear day by overlaying a cloud-colored image with a certain opacity.

The reason I want to use a dataloader to do this operation, and not to pre-bake the training data, is because it ensures the same frame is used for the "low" and "high" quality image, and the clouds are random. It also allows for me to add data to future training batches without having to re-run a script that makes low-high image pairs.

Once an image has been trained on, I want to overlay the data segment of the original frame over the cleaned up image. This way, the original metadata is not messed with by the ViT. Having the ViT not have to deal with cleaning up the clouds and maintaining clear text will help improve accuracy, as the ViT has one less thing to do.
#+end_quote

@TODO:
1. Go from videos to images
   - FFmpeg can do this: ~ffmpeg -i input.avi output_%3d.png~
2. Go from images to chunks for training
   - The frames are already 608x608. What if that's the chunk? The patch size could be 16 or 19
3. ViT to train
   - Includes two transforms in the dataloader:
     - one for clear data (i.e. it's unmodified), and
     - one for clouds (i.e. it's faking clouds over the same image)
4. A script to take a video, make it a bunch of images, then chunks(?), then enhance, then reverse the process
5. A . . . GUI?
